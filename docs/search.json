[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/05-hypothesis/index.html",
    "href": "posts/05-hypothesis/index.html",
    "title": "Mini Project 5: Advantages and Drawbacks of Using p-values",
    "section": "",
    "text": "I think that the authors mean it will be more important to effectively convey and understand why they are getting the results they are as the “statistical significance” rule for drawing conclusions is used less. All the research and experimentation won’t be summed up in a single value. Given their hypothesis and thoughts on what is happening, a p-value might be used as more of a measurement of what happened and they can explain why, but not as an explanation for a conclusion. In my mind, statistical thinking pertains more to using the data to try and explain what happened/explain a conclusion. That’s a very vague statement, but I mean instead of saying the p-value is large so the results are not statistically significant, we can look at this value and the actual results/data and make a judgement that acknowledges the uncertainty and the context of the data. Not just dismissing what you find because the p-value says it’s not significant.\nThey talk a lot about treating p-values as continuous values. Although the whole point of the editorial is that p-values shouldn’t be a judge of “significance,” it is still a quantitative metric that can help give a sense of where the null hypothesis stands relative to the data. By dichotomizing the p-value with significance and insignificance, we are essentially ignoring what the data is actually saying and narrowing it all down to this single boundary. There is such a hard cut-off for p-values between what’s deemed statistically significant and insignificant, it leaves no wiggle room, and fractions of decimal points can lead to complete opposite conclusions. The dichotomization also can lead to ignoring results that are still meaningful. If the p-value misses a cut-off by 0.001, it is deemed insignificant and not in support of the research question, but in reality there is only a marginal difference between a p-value 0.001 below the threshold that was deemed significant.\nI agree. In the case of “insignificance” where the p-value misses the arbitrary threshold, it doesn’t mean that the results of the project were not meaningful or important. Also, for all we know there is “significance” and it’s just this p-value saying there isn’t. This is likely over simplifying whatever actually happened. When deciding what results to present or highlight in publishing, I think it should be based more on how the results are explained and interpreted. Even in the case where the p-value supports the hypothesis, an explanation for what you found should still be required to attribute it to your hypothesis, not just because the number said so. When only “significant” results are published, other people can’t learn from whatever was found. Results that (allegedly) don’t support your hypothesis are still results.\nI agree. Research and data have different contexts, so they can’t all be looked at in the same way. Different areas of research (ex. pharmaceuticals, psychology, economics) may have different boundaries for what should be considered data “in support” of your hypothesis, and even within these disciplines themselves there will surely be variation. Different disciplines ask different research questions and deal with different data, so it makes sense that they should be dealt with in different ways. To create a one-size-fits-all approach would likely cause some sort of loss of information or too broad of a generalization to be the best system.\nI think statistical thoughtfulness means thinking carefully about what results mean, analyzing the data in context and with purpose, not just blindly using a p-value to determine your final conclusion. Even before looking at results, it can be acknowledging limitations or variation in the research and using that in interpreting the results. Statistical thoughtfulness can be demonstrated in an analysis by being deliberate in your choices, such as how you design the research/experiment, pre-determining the type of results you’re looking for (i.e. what supports your hypothesis), and how you communicate the results. For example, relating back to the p-value, explaining your level of confidence in your results instead of just talking about how the p-value is small therefore we were right, shows that you understand the results and can interpret them in the context in a way that makes sense and is transparent.\nI think they believe the problem is that they are somewhat deterministic words. For example, saying we’re x% “confident” that the true value is in this interval may be misleading because the interval was created using a specific method on a specific set of data. Using words like significance and confidence seems like we’re fairly certain that this is actually what is happening on the general level and is often without context. Instead of drawing hard conclusions (ex. is significant/insignificant), you can say that based on the data and the model, the interval/p-value/… is compatible with having x quantitative effect. Using the word compatible encourages more of that statistical thinking over making dichotomous conclusions with a p-value. In general, I think that a terminology change would not be an immediate fix, but may steer researchers towards more thought-out and contextualized conclusions.\nSection 5, paragraph 2: Why is eliminating the use of p-values as a truth arbiter so hard? “The basic explanation is neither philosophical nor scientific, but sociologic; everyone uses them,” says Goodman (2019). “It’s the same reason we can use money. When everyone believes in something’s value, we can use it for real things; money for food, and p-values for knowledge claims, publication, funding, and promotion. It doesn’t matter if the p-value doesn’t mean what people think it means; it becomes valuable because of what it buys.”\n\nThis section stood out to me because of the comparison to money. If you think about it, money is just a made-up concept that we use to trade for things. Objectively, money has no value. It doesn’t do anything on its own, but you can use it to do things you want because of its alleged value. It’s become this huge construct that basically runs the world because we follow the system. Way back before currency was a thing, people just traded directly for things, and then one day there is this coin that’s being used instead. I’m sure it didn’t go smoothly the first time the first person tried to get something of value using some random coins since a metal disk doesn’t do anything. But wait, they can use those coins to get something else. How? Because we’re saying they’re valuable. The p-value has become accepted as this benchmark for whether your research “proved” what it was trying to prove because we’ve given it that value. Like this whole editorial says, a p-value doesn’t technically actually mean some hypothesis has been “proved” or anything, but since we’ve given it that power, it is valuable. And I can’t see society stop using money all of a sudden, so if we want to take power away from the p-value, it will probably have to be a gradual change."
  },
  {
    "objectID": "posts/01-simulation/index.html",
    "href": "posts/01-simulation/index.html",
    "title": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "",
    "text": "Using simulation to investigate the sampling distributions for the minimum and maximum of samples taken from different populations."
  },
  {
    "objectID": "posts/01-simulation/index.html#population-distributions",
    "href": "posts/01-simulation/index.html#population-distributions",
    "title": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "Population Distributions",
    "text": "Population Distributions"
  },
  {
    "objectID": "posts/01-simulation/index.html#normal-distribution",
    "href": "posts/01-simulation/index.html#normal-distribution",
    "title": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "Normal Distribution",
    "text": "Normal Distribution\n\\(Y \\sim N(\\mu = 10, \\sigma^2 = 4)\\)\n\nSimulate the Distribution: Minimum \\(Y_{min}\\)\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_samp_min_norm &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min_norm(mu = mu, sigma = sigma, n = n))\n\n## each number represents the sample mean from __one__ sample.\nnorm_mins_df &lt;- tibble(mins)\n\nnorm_mins_plot &lt;- ggplot(data = norm_mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = \"normal(10, 4)\")\n\nnorm_mins_summ &lt;- norm_mins_df |&gt;\n  summarise(mean_samp_dist = mean(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n\n\nSimulate the Distribution: Maximum \\(Y_{max}\\)\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_samp_max_norm &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmaxes &lt;- map_dbl(1:nsim, \\(i) generate_samp_max_norm(mu = mu, sigma = sigma, n = n))\n\n## each number represents the sample mean from __one__ sample.\nnorm_maxes_df &lt;- tibble(maxes)\n\nnorm_maxes_plot &lt;- ggplot(data = norm_maxes_df, aes(x = maxes)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maxes\",\n       title = \"normal(10, 4)\")\n\nnorm_maxes_summ &lt;- norm_maxes_df |&gt;\n  summarise(mean_samp_dist = mean(maxes),\n            var_samp_dist = var(maxes),\n            sd_samp_dist = sd(maxes))"
  },
  {
    "objectID": "posts/01-simulation/index.html#uniform-distribution",
    "href": "posts/01-simulation/index.html#uniform-distribution",
    "title": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\n\\(Y \\sim Unif(\\theta_{1} = 7, \\theta_{2} = 13)\\)\n\nSimulate the Distribution: Minimum \\(Y_{min}\\)\n\nn &lt;- 5            # sample size\ntheta1 &lt;- 7          \ntheta2 &lt;- 13    \n\ngenerate_samp_min_unif &lt;- function(theta1, theta2, n) {\n  \n  single_sample &lt;- runif(n, theta1, theta2)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min_unif(theta1 = theta1, theta2 = theta2, n = n))\n\n## each number represents the sample mean from __one__ sample.\nunif_mins_df &lt;- tibble(mins)\n\nunif_mins_plot &lt;- ggplot(data = unif_mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = \"uniform(7, 13)\")\n\nunif_mins_summ &lt;- unif_mins_df |&gt;\n  summarise(mean_samp_dist = mean(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n\n\nSimulate the Distribution: Maximum \\(Y_{max}\\)\n\nn &lt;- 5            # sample size\ntheta1 &lt;- 7         \ntheta2 &lt;- 13        \n\ngenerate_samp_max_unif &lt;- function(theta1, theta2, n) {\n  \n  single_sample &lt;- runif(n, theta1, theta2)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmaxes &lt;- map_dbl(1:nsim, \\(i) generate_samp_max_unif(theta1 = theta1, theta2 = theta2, n = n))\n\n## each number represents the sample mean from __one__ sample.\nunif_maxes_df &lt;- tibble(maxes)\n\nunif_maxes_plot &lt;- ggplot(data = unif_maxes_df, aes(x = maxes)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maxes\",\n       title = \"uniform(7, 13)\")\n\nunif_maxes_summ &lt;- unif_maxes_df |&gt;\n  summarise(mean_samp_dist = mean(maxes),\n            var_samp_dist = var(maxes),\n            sd_samp_dist = sd(maxes))"
  },
  {
    "objectID": "posts/01-simulation/index.html#exponential-distribution",
    "href": "posts/01-simulation/index.html#exponential-distribution",
    "title": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "Exponential Distribution",
    "text": "Exponential Distribution\n\\(Y \\sim Exp(\\lambda = 0.5)\\)\n\nSimulate the Distribution: Minimum \\(Y_{min}\\)\n\nn &lt;- 5       # sample size\nlambda &lt;- 0.5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\ngenerate_samp_min_exp &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min_exp(lambda = lambda, n = n))\n\n## each number represents the sample mean from __one__ sample.\nexp_mins_df &lt;- tibble(mins)\n\nexp_mins_plot &lt;- ggplot(data = exp_mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = \"exp(0.5)\")\n\nexp_mins_summ &lt;- exp_mins_df |&gt;\n  summarise(mean_samp_dist = mean(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n\n\nSimulate the Distribution: Maximum \\(Y_{max}\\)\n\nn &lt;- 5       # sample size\nlambda &lt;- 0.5\nmu &lt;- 1 / lambda   # population mean\nsigma &lt;- sqrt(1 / lambda ^ 2)  # population standard deviation\n\ngenerate_samp_max_exp &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmaxes &lt;- map_dbl(1:nsim, \\(i) generate_samp_max_exp(lambda = lambda, n = n))\n\n## each number represents the sample mean from __one__ sample.\nexp_maxes_df &lt;- tibble(maxes)\n\nexp_maxes_plot &lt;- ggplot(data = exp_maxes_df, aes(x = maxes)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maxes\",\n       title = \"exp(0.5)\")\n\nexp_maxes_summ &lt;- exp_maxes_df |&gt;\n  summarise(mean_samp_dist = mean(maxes),\n            var_samp_dist = var(maxes),\n            sd_samp_dist = sd(maxes))"
  },
  {
    "objectID": "posts/01-simulation/index.html#beta-distribution",
    "href": "posts/01-simulation/index.html#beta-distribution",
    "title": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "Beta Distribution",
    "text": "Beta Distribution\n\\(Y \\sim Beta(\\alpha = 8, \\beta = 2)\\)\n\nSimulate the Distribution: Minimum \\(Y_{min}\\)\n\nn &lt;- 5       # sample size\nalpha &lt;- 8\nbeta &lt;- 2\n\ngenerate_samp_min_beta &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min_beta(alpha = alpha, beta = beta, n = n))\n\n## each number represents the sample mean from __one__ sample.\nbeta_mins_df &lt;- tibble(mins)\n\nbeta_mins_plot &lt;- ggplot(data = beta_mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = \"beta(8, 2)\")\n\nbeta_mins_summ &lt;- beta_mins_df |&gt;\n  summarise(mean_samp_dist = mean(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n\n\nSimulate the Distribution: Maximum \\(Y_{max}\\)\n\nn &lt;- 5       # sample size\nalpha &lt;- 8\nbeta &lt;- 2\n\ngenerate_samp_max_beta &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmaxes &lt;- map_dbl(1:nsim, \\(i) generate_samp_max_beta(alpha = alpha, beta = beta, n = n))\n\n## each number represents the sample mean from __one__ sample.\nbeta_maxes_df &lt;- tibble(maxes)\n\nbeta_maxes_plot &lt;- ggplot(data = beta_maxes_df, aes(x = maxes)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maxes\",\n       title = \"beta(8, 2)\")\n\nbeta_maxes_summ &lt;- beta_maxes_df |&gt;\n  summarise(mean_samp_dist = mean(maxes),\n            var_samp_dist = var(maxes),\n            sd_samp_dist = sd(maxes))"
  },
  {
    "objectID": "posts/01-simulation/index.html#simulated-distributions-of-sample-minimums",
    "href": "posts/01-simulation/index.html#simulated-distributions-of-sample-minimums",
    "title": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "Simulated Distributions of Sample Minimums",
    "text": "Simulated Distributions of Sample Minimums"
  },
  {
    "objectID": "posts/01-simulation/index.html#simulated-distributions-of-sample-maximums",
    "href": "posts/01-simulation/index.html#simulated-distributions-of-sample-maximums",
    "title": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "Simulated Distributions of Sample Maximums",
    "text": "Simulated Distributions of Sample Maximums"
  },
  {
    "objectID": "posts/01-simulation/index.html#results",
    "href": "posts/01-simulation/index.html#results",
    "title": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "Results",
    "text": "Results\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n7.6721479\n7.9906366\n0.3948871\n0.6468993\n\n\n\\(\\text{E}(Y_{max})\\)\n12.3195106\n12.0186568\n4.6088644\n0.9217724\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n1.2966635\n0.8447587\n0.3929804\n0.105978\n\n\n\\(\\text{SE}(Y_{max})\\)\n1.3333167\n0.8330108\n2.4335272\n0.0455276"
  },
  {
    "objectID": "posts/01-simulation/index.html#discussion",
    "href": "posts/01-simulation/index.html#discussion",
    "title": "Mini Project 1: Sampling Distribution of the Sample Minimum and Maximum",
    "section": "Discussion",
    "text": "Discussion\n\nUseful Situations\nThe normal distribution should be used when modeling data that is approximately symmetric and fairly centered around a certain value. That is, the “extreme” values are approximately equally likely to occur, while lots of values cluster around an average (ex. people’s heights). A uniform distribution should be used when all values in a certain range are equally likely to occur (ex. rolling a die). An exponential distribution should be used when modeling data that is essentially “time” between an event occurring, while the rate parameter \\(\\lambda\\) is the average rate at which this event occurs in a given time (ex. lifetimes). Lastly, a beta distribution should be used when values fall between 0 and 1, which typically represents a percentage (ex. likelihood of 60% of people liking cilantro).\n\n\n\\(SE(Y_{min})\\) vs \\(SE(Y_{max})\\)\nFor the normal and uniform population models, \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) are extremely close. For the exponential and beta distributions, they are very different. Based on the observed values and visual representations of the population distributions, it seems that a general rule could be that \\(SE(Y_{min}) \\approx SE(Y_{max})\\) if the population distribution is symmetrical.\n\n\nPDF for the Exponential Distribution\n\\(Y \\sim Exp(\\lambda = 0.5)\\)\n\nMinimum\n\\(f(y) = \\lambda e^{-\\lambda y}\\)\n\\(f(y) = 0.5e^{-0.5y}\\) where \\(y &gt; 0\\)\nFind CDF, F(y):\n\\(F(y) = \\int_{0}^{y}0.5e^{-0.5y} dy = 1 - e^{-0.5y}\\)\nUse CDF and PDF for Y to find PDF for Minimum:\n\\(f_{min}(y_{min}) = n(1-F(y))^{n-1}f(y)\\)\n\\(f_{min}(y_{min}) = n(1-(1 - e^{-0.5y})^{n-1}0.5e^{-0.5y}\\)\n\\(f_{min}(y_{min}) = n(e^{-0.5y})^{n-1}0.5e^{-0.5y}\\)\n\\(f_{min}(y_{min}) = 0.5ne^{-0.5yn}\\)\nif n = 5, \\(f_{min}(y_{min}) = 2.5e^{-2.5y}\\)\nExpectation for Minimum:\n\\(E(Y_{min}) = \\int_{0}^{\\infty}y_{min}f_{min}(y_{min})dy_{min}\\)\n\\(E(Y_{min}) = \\int_{0}^{\\infty}y_{min}2.5e^{-2.5y_{min}}dy_{min}\\) (use Symbolab to calculate)\n\\(E(Y_{min}) = 0.4\\)\nPDF for Minimum with Expected Value\n\n\n\n\n\n\n\n\n\nSE for Minimum:\n\\(E(Y_{min}^2) = \\int_{0}^{\\infty}y_{min}^2f_{min}(y_{min})dy_{min}\\)\n\\(E(Y_{min}^2) = \\int_{0}^{\\infty}y_{min}^22.5e^{-2.5y_{min}}dy_{min}\\)\n\\(E(Y_{min}^2) = 0.32\\) (use Symbolab to calculate)\n\\(Var(Y_{min}) = E(Y_{min}^2) - [E(Y_{min})]^2\\)\n\\(Var(Y_{min}) = 0.32 - 0.4^2\\)\n\\(Var(Y_{min}) = 0.16\\)\n\\(SE(Y_{min}) = \\sqrt{0.16} = 0.40\\)\n\n\nMaximum\nUse CDF and PDF for Y to find PDF for Maximum:\n\\(f_{max}(y_{max}) = n(F(y))^{n-1}f(y)\\)\n\\(f_{max}(y_{max}) = n(1 - e^{-0.5y})^{n-1}0.5e^{-0.5y}\\)\nif n = 5, \\(f_{max}(y_{max}) = 2.5(1 - e^{-0.5y})^{4}e^{-0.5y}\\)\nExpectation for Maximum:\n\\(E(Y_{max}) = \\int_{0}^{\\infty}y_{max}f_{max}(y_{max})dy_{max}\\)\n\\(E(Y_{max}) = \\int_{0}^{\\infty}y_{max}2.5(1 - e^{-0.5y_{max}})^{4}e^{-0.5y_{max}}dy_{max}\\) (use Symbolab to calculate)\n\\(E(Y_{max}) = 4.567\\)\nPDF for Maximum with Expected Value\n\n\n\n\n\n\n\n\n\nSE for Maximum:\n\\(E(Y_{max}^2) = \\int_{0}^{\\infty}y_{max}^2f_{max}(y_{max})dy_{max}\\)\n\\(E(Y_{max}^2) = \\int_{0}^{\\infty}y_{max}^22.5(1 - e^{-0.5y_{max}})^{4}e^{-0.5y_{max}}dy_{max}\\) (use Symbolab to calculate)\n\\(E(Y_{max}^2) = 26.709\\)\n\\(Var(Y_{max}) = E(Y_{max}^2) - [E(Y_{max})]^2\\)\n\\(Var(Y_{max}) = 26.709 - 4.567^2\\)\n\\(Var(Y_{max}) = 5.85151\\)\n\\(SE(Y_{max}) = \\sqrt{5.85151} = 2.418989\\)\nFor both the minimum and maximum, the expected values and standard errors are very close to the results of the simulation.\n\nTable of Results\n\n\n\n\n\n\n\n\n\\(\\text{Simulation}\\)\n\\(\\text{PDF}\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n0.3948871\n0.4\n\n\n\\(\\text{E}(Y_{max})\\)\n4.6088644\n4.567\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n0.3929804\n0.4\n\n\n\\(\\text{SE}(Y_{max})\\)\n2.4335272\n2.418989"
  },
  {
    "objectID": "posts/04-bayesian/index.html",
    "href": "posts/04-bayesian/index.html",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "",
    "text": "Using Bayesian analysis to investigate the probability that Nadal wins a point on his own serve against Novak Djokovic at the French Open."
  },
  {
    "objectID": "posts/04-bayesian/index.html#priors",
    "href": "posts/04-bayesian/index.html#priors",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "Priors",
    "text": "Priors\nSince our unknown parameter is \\(p\\), which is always between 0 and 1, I will use Beta as the prior distribution.\nFor the non-informative scenario 1, a reasonable prior for \\(p\\) is \\(Uniform(0,1) = Beta(1,1)\\) since it gives an equal probability to all values between 0 and 1, which reflects the lack of knowledge.\nGiven the information in scenario 2, we can find good parameters for the prior that fit the given constraints.\n\ntarget_mean &lt;- 46/66\n\nalphas &lt;- seq(0.1, 60, length.out = 500)\nbetas &lt;- (1-46/66)*alphas/(46/66)\n\nparam_df &lt;- tibble(alphas, betas)\n\nparam_df &lt;- param_df |&gt; mutate(vars = \n                    (alphas*betas)/((alphas + betas)^2*(alphas + betas + 1)))\n\n\ntarget_var &lt;- 0.05657^2\n\nparam_df &lt;- param_df |&gt; mutate(dist_to_target = abs(vars - target_var))\n\nparam_df |&gt; filter(dist_to_target == min(dist_to_target))\n\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1   45.4  19.7 0.00320     0.00000374\n\n\n\\(Beta(45.36,19.72)\\)\nGiven the information in scenario 3, we can find good parameters for the prior that fit the given constraints.\nGenerate possible pairs of alpha and beta values that result an a mean of 0.75\nRecall that the mean of a Beta distribution is calculated using: \\(\\frac{\\alpha}{\\alpha + \\beta}\\)\n\nalpha &lt;- seq(0.001, 300, length.out = 1000)\nbeta &lt;- 0.25*alpha/0.75\npossible_params &lt;- tibble(alpha,beta)\n\nGiven we are “almost sure” that Nadal wins no less than 70% of his points on serve against Djokovic, we want \\(P(p &lt; 0.70)\\) to be a small value, and I am going with 0.02\n\npossible_params %&gt;% \n  mutate(probs = pbeta(0.70, alpha, beta)) %&gt;%\n  mutate(dist_to_target = abs(probs - 0.02)) %&gt;%\n  filter(dist_to_target == min(dist_to_target))\n\n# A tibble: 1 × 4\n  alpha  beta  probs dist_to_target\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;\n1  252.  83.9 0.0200      0.0000289\n\n\n\\(Beta(251.65,83.88)\\)\n\n\n\n\n\n\n\n\n\n\nNon-Informative Prior\nInformative Prior 1\nInformative Prior 2\n\n\n\n\n\\(\\alpha\\)\n1\n45.35511\n251.6518\n\n\n\\(\\beta\\)\n1\n19.71961\n83.88394\n\n\nMean\n0.5\n0.69697\n0.75"
  },
  {
    "objectID": "posts/04-bayesian/index.html#data",
    "href": "posts/04-bayesian/index.html#data",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "Data",
    "text": "Data\nFrom class, we know we can update the prior distribution based on our data. If \\(y_{obs}\\) is the number of points won:\n\\(g(p|y_{obs}) = Beta(y_{obs} + \\alpha, n - y_{obs} + \\beta)\\)\n\\(y_{obs} = 56\\)\n\\(n = 84\\)\nThe mean is equal to \\(\\frac{\\alpha}{\\alpha + \\beta}\\):\nCarry out the math…\n\n\n\n\n\n\n\n\n\n\nNon-Informative Posterior\nInformative Posterior 1\nInformative Posterior 2\n\n\n\n\n\\(\\alpha\\)\n57\n101.3551\n95.03965\n\n\n\\(\\beta\\)\n29\n47.71961\n41.01322\n\n\nMean\n0.6627907\n0.6798947\n0.7333149\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-Informative Posterior\nInformative Posterior 1\nInformative Posterior 2\n\n\n\n\nPrior Mean\n0.5\n0.69697\n0.75\n\n\nPosterior Mean\n0.6627907\n0.6798947\n0.6985494\n\n\n\nAll of the posterior means are between their prior mean and our value from the data (\\(\\frac{56}{84} = 0.667\\)), which makes sense.\n90% Credible Intervals\n\n# non informative\nc(qbeta(0.05, 57, 29),qbeta(0.95, 57, 29))\n\n[1] 0.5772453 0.7440061\n\n# informative 1\nc(qbeta(0.05, 101.3551, 47.71961), qbeta(0.95, 101.3551, 47.71961))\n\n[1] 0.6158797 0.7411561\n\n# informative 2\nc(qbeta(0.05, 307.6518, 111.8839), qbeta(0.95, 307.6518, 111.8839))\n\n[1] 0.6972162 0.7681476"
  },
  {
    "objectID": "posts/04-bayesian/index.html#comparing-the-posteriors",
    "href": "posts/04-bayesian/index.html#comparing-the-posteriors",
    "title": "Mini Project 4: Bayesian Analysis",
    "section": "Comparing the Posteriors",
    "text": "Comparing the Posteriors\nThe three posterior distributions are all different because we started with different priors (different \\(\\alpha\\) and \\(\\beta\\) values). In class we figured out how to update our distribution using our data:\n\\(g(p|y_{obs}) = Beta(y_{obs} + \\alpha, n - y_{obs} + \\beta)\\)\nSo we can see that although each prior is updated in the same way, the initial \\(\\alpha\\) and \\(\\beta\\) values will have a strong influence on the resulting posterior.\n\nChoosing a Posterior\nBased on the three posteriors, I would choose the first informative one. Although the second informative prior has the lowest variance by a fair margin (see below), it is only based on the claims of the commentator. So yes the distribution has a small variance, but this smaller range of values that the 90% credible interval covers may be inaccurate. The first informative one’s prior was based on real (?) data from previous matches between Nadal and Djokovic, and for this reason I would pick it over both of the other options. It reflects known past performance of Nadal versus Djokovic, and based on the real data we have, the interval seems more reasonable. This first informative scenario gives us data to use where p = 0.667, and this value is not even in the credible interval for the second informative scenario.\n\n\nPosterior Variance\n\\(Variance = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\)\n\nvariance &lt;- function(alpha, beta) {\n  var = (alpha*beta)/((alpha + beta)^2*(alpha + beta + 1))\n  return (var)\n}\n# non-informative\nvariance(noninformative_alpha, noninformative_beta)\n\n[1] 0.002568956\n\n# informative 1\nvariance(informative_alpha1, informative_beta1)\n\n[1] 0.001450197\n\n# informative 2\nvariance(informative_alpha2, informative_beta2)\n\n[1] 0.0004650358\n\n\nThe variance for the second informative posterior is lower than the variance for the other two because its prior started with a much lower variance. From the first graph you can see that the curve for the second prior is much taller and narrower, indicating a low variance. This makes sense because we knew the peak had to be “centered” over 0.75 and have only 2% of the density below 0.70, which is not far from 0.75. So although the distribution was adjusted by the observed data, 0.667 is not too far from 0.75, so the distribution stayed narrow. Since the distribution started with a relatively high confidence in Nadal’s performance, it remained high."
  },
  {
    "objectID": "posts/03-intervals/index.html",
    "href": "posts/03-intervals/index.html",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "",
    "text": "Investigating confidence intervals when we violate assumptions.\nSample sizes: 5, 25, 500\nProportions: 0.47, 0.88"
  },
  {
    "objectID": "posts/03-intervals/index.html#function-for-generating-sample-proportions",
    "href": "posts/03-intervals/index.html#function-for-generating-sample-proportions",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Function for generating sample proportions",
    "text": "Function for generating sample proportions\n\ngenerate_samp_prop &lt;- function(n, p, alpha) {\n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  \n  ## confidence interval\n  lb &lt;- phat - qnorm(1 - alpha / 2) * sqrt(phat * (1 - phat) / n)\n  ub &lt;- phat + qnorm(1 - alpha / 2) * sqrt(phat * (1 - phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}"
  },
  {
    "objectID": "posts/03-intervals/index.html#sample-size-500-proportion-0.47",
    "href": "posts/03-intervals/index.html#sample-size-500-proportion-0.47",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Sample Size 500, Proportion 0.47",
    "text": "Sample Size 500, Proportion 0.47\n\nn &lt;- 500   # sample size\np &lt;- 0.47   # population proportion\nalpha &lt;- 0.1 \n\n## number of CIs\nn_sim &lt;- 5000\n\nprop_ci_df_ll &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\n\n\nsummary_large_low &lt;- prop_ci_df_ll %&gt;%\n  mutate(width = ub - lb,\n         covered = if_else(p &gt; lb & p &lt; ub, 1, 0)) %&gt;%\n  summarise(avg_width = round(mean(width), 4), cov_rate = mean(covered))\n\nLarge sample assumption:\n500(0.47) = 235 &gt; 10 \\(\\therefore\\) satisfied\n500(1 - 0.47) = 265 &gt; 10 \\(\\therefore\\) satisfied"
  },
  {
    "objectID": "posts/03-intervals/index.html#sample-size-500-proportion-0.88",
    "href": "posts/03-intervals/index.html#sample-size-500-proportion-0.88",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Sample Size 500, Proportion 0.88",
    "text": "Sample Size 500, Proportion 0.88\n\nn &lt;- 500   # sample size\np &lt;- 0.88  # population proportion\nalpha &lt;- 0.1 \n\n## number of CIs\nn_sim &lt;- 5000\n\nprop_ci_df_lh &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\n\n\nsummary_large_high &lt;- prop_ci_df_lh %&gt;%\n  mutate(width = ub - lb,\n         covered = if_else(p &gt; lb & p &lt; ub, 1, 0)) %&gt;%\n  summarise(avg_width = round(mean(width), 4), cov_rate = mean(covered))\n\nLarge sample assumption:\n500(0.88) = 440 &gt; 10 \\(\\therefore\\) satisfied\n500(1 - 0.88) = 60 &gt; 10 \\(\\therefore\\) satisfied"
  },
  {
    "objectID": "posts/03-intervals/index.html#sample-size-25-proportion-0.47",
    "href": "posts/03-intervals/index.html#sample-size-25-proportion-0.47",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Sample Size 25, Proportion 0.47",
    "text": "Sample Size 25, Proportion 0.47\n\nn &lt;- 25   # sample size\np &lt;- 0.47  # population proportion\nalpha &lt;- 0.1 \n\n## number of CIs\nn_sim &lt;- 5000\n\nprop_ci_df_ml &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\n\n\nsummary_mid_low &lt;- prop_ci_df_ml %&gt;%\n  mutate(width = ub - lb,\n         covered = if_else(p &gt; lb & p &lt; ub, 1, 0)) %&gt;%\n  summarise(avg_width = round(mean(width), 4), cov_rate = mean(covered))\n\nLarge sample assumption:\n25(0.47) = 11.75 &gt; 10 \\(\\therefore\\) satisfied\n25(1 - 0.47) = 13.25 &gt; 10 \\(\\therefore\\) satisfied"
  },
  {
    "objectID": "posts/03-intervals/index.html#sample-size-25-proportion-0.88",
    "href": "posts/03-intervals/index.html#sample-size-25-proportion-0.88",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Sample Size 25, Proportion 0.88",
    "text": "Sample Size 25, Proportion 0.88\n\nn &lt;- 25   # sample size\np &lt;- 0.88  # population proportion\nalpha &lt;- 0.1 \n\n## number of CIs\nn_sim &lt;- 5000\n\nprop_ci_df_mh &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\n\n\nsummary_mid_high &lt;- prop_ci_df_mh %&gt;%\n  mutate(width = ub - lb,\n         covered = if_else(p &gt; lb & p &lt; ub, 1, 0)) %&gt;%\n  summarise(avg_width = round(mean(width), 4), cov_rate = mean(covered))\n\nLarge sample assumption:\n25(0.88) = 22 &gt; 10\n25(1 - 0.88) = 3 &lt; 10 \\(\\therefore\\) not satisfied"
  },
  {
    "objectID": "posts/03-intervals/index.html#sample-size-5-proportion-0.47",
    "href": "posts/03-intervals/index.html#sample-size-5-proportion-0.47",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Sample Size 5, Proportion 0.47",
    "text": "Sample Size 5, Proportion 0.47\n\nn &lt;- 5   # sample size\np &lt;- 0.47  # population proportion\nalpha &lt;- 0.1 \n\n## number of CIs\nn_sim &lt;- 5000\n\nprop_ci_df_sl &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\n\n\nsummary_small_low &lt;- prop_ci_df_sl %&gt;%\n  mutate(width = ub - lb,\n         covered = if_else(p &gt; lb & p &lt; ub, 1, 0)) %&gt;%\n  summarise(avg_width = round(mean(width), 4), cov_rate = mean(covered))\n\nLarge sample assumption:\n5(0.47) = 2.35 &lt; 10 \\(\\therefore\\) not satisfied\n5(1 - 0.47) = 2.65 &lt; 10 \\(\\therefore\\) not satisfied"
  },
  {
    "objectID": "posts/03-intervals/index.html#sample-size-5-proportion-0.88",
    "href": "posts/03-intervals/index.html#sample-size-5-proportion-0.88",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Sample Size 5, Proportion 0.88",
    "text": "Sample Size 5, Proportion 0.88\n\nn &lt;- 5   # sample size\np &lt;- 0.88  # population proportion\nalpha &lt;- 0.1 \n\n## number of CIs\nn_sim &lt;- 5000\n\nprop_ci_df_sh &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n = n, p = p, alpha = alpha)) |&gt;\n  bind_rows()\n\n\nsummary_small_high &lt;- prop_ci_df_sh %&gt;%\n  mutate(width = ub - lb,\n         covered = if_else(p &gt; lb & p &lt; ub, 1, 0)) %&gt;%\n  summarise(avg_width = round(mean(width), 4), cov_rate = mean(covered))\n\nLarge sample assumption:\n5(0.88) = 4.4 &lt; 10 \\(\\therefore\\) not satisfied\n5(1 - 0.88) = 0.6 &lt; 10 \\(\\therefore\\) not satisfied\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 5\\)\n\\(n = 25\\)\n\\(n = 500\\)\n\n\n\n\n\\(p = 0.47\\)\nCoverage Rate\n0.8094\n0.8888\n0.8978\n\n\n\\(p = 0.88\\)\nCoverage Rate\n0.47\n0.796\n0.8804\n\n\n\n\n\n\n\n\n\n\\(p = 0.47\\)\nAverage Width\n0.6359\n0.3215\n0.0734\n\n\n\\(p = 0.88\\)\nAverage Width\n0.3003\n0.2003\n0.0477"
  },
  {
    "objectID": "posts/03-intervals/index.html#findings",
    "href": "posts/03-intervals/index.html#findings",
    "title": "Mini Project 3: Simulation to Investigate Confidence Intervals",
    "section": "Findings",
    "text": "Findings\nThe large sample assumption is satisfied for three out of the six settings (both n = 500, n = 25 where p = 0.47). For the three settings in which it is satisfied, the coverage rate is about 0.9 or very close to it, which is what we wanted given that we were attempting to simulate a 90% confidence interval. For the cases in which the large sample assumption is not satisfied, the coverage rates are less than 0.9.\nFor the interval widths, the general trend is that the width decreases as sample size increases. The width of the interval is determined by the margin of error term of the confidence interval calculation \\(z_{1-\\alpha/2}*\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\). The \\(z_{1-\\alpha/2}\\) part is the same for all settings, but the \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) part varies between them. When n is small, this increases the value of this term, which makes sense because when the sample size is small each 1 and 0 (success/failure) has a greater impact on that sample’s proportion, so it could tend to change a lot. The resulting high margin of error and wide interval helps account for this tendency. Back to the coverage rate, the \\(\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) only approximates the \\(SE(\\hat{p})\\) if n is large, and since it isn’t for the low coverage cases, it makes sense that the interval itself is off.\nDespite not satisfying the large sample assumption, the coverage rate for the n = 5 with p = 0.47 setting actually wasn’t as far off as I thought it would be, and this is seen in its wide interval. The width of about 0.63 (depends on the simulation) covers a lot of the whole range of 0-1, which matches the high coverage rate. With a sample size of 5, there are only 6 possible \\(\\hat{p}\\) values we can get (0, \\(\\frac{1}{5}\\),…,\\(\\frac{5}{5}\\)), but since it’s almost equally likely to get a 0 or 1 in this setting, you don’t really tend to get one over the other, so maybe the interval “widens” to account for this uncertainty. This is another instance of what I explained above in the interval width section, but very extreme since the population proportion is close to 0.5, which maximizes the \\(\\hat{p}(1-\\hat{p})\\) part. For the n = 5 and p = 0.88 setting I think the interval might be narrower than the p = 0.47 one because of the population proportion. With p = 0.88, it is not that unrealistic to get five 1’s in the sample, so the margin of error for those becomes 0 since \\(\\hat{p}(1-\\hat{p})\\) will be 0. This makes the upper and lower bounds both 1, which makes the interval width also 0. If this happens enough times, this could significantly decrease the average width. I actually checked for one of the simulations and it was 2662 times, over half of the simulated samples!\nSomething to note is that the “bounds” of the intervals were sometimes less than 0 or greater than 1. Both could never happen, but I think the technical width provided by the margin of error and not the actual value of the bounds is what was important about this project. But overall, the general trend is that the coverage rate is lower than the expected amount when the large sample assumption is not satisfied, and interval width tends to increase as sample size decreases."
  },
  {
    "objectID": "posts/02-estimation/index.html",
    "href": "posts/02-estimation/index.html",
    "title": "Mini Project 2: A Meaningful Story",
    "section": "",
    "text": "Appleton Arena is home to the St. Lawrence University men’s ice hockey team. Every home weekend it is packed to the brim, the cacophonous roar of fans making it impossible to hear anything but the sound of players hitting the boards and the referee’s whistle. Students arrive early in an attempt at getting a seat in the student section, also known as the Marsh Pit, and there is often not a spare inch of room in the stands. Appleton Arena is also home to the St. Lawrence University women’s ice hockey team. The team sees a decent number of local fans, but often the wood of the vintage bench seating is exposed between sparse groups of spectators.\nIn an effort to improve viewership of women’s hockey games, Laura, the social media manager for the team, wants to do some kind of promotion. Her idea is to give out free pub cookies at the next home game in hopes that word will spread about the delicious incentive and more people will turn up for the next game. But how many cookies will be needed? Too few and the people will not be happy that they didn’t get their decadent dessert, too many and there will be food waste. In order to figure out how many cookies she needs, she is going to find the typical attendance and go from there. To find these numbers, she consults Brennan, the hockey operations manager for the team. “Can we just use the attendance from the last game?” asks Laura. Brennan tells Laura that we can treat the attendance at a given game as a random variable. “The attendance at a given game will vary,” says Brennan to Laura, “and we have to account for this variability when making this decision on attendance, so we can’t just look at the numbers from last game because I’m sure it was different the game before that. We can try to find a good way to estimate the mean, which may give as a better representation of our typical attendance.” Laura has a good idea of where this is going now. If she takes random samples of games, she could find a way to use them to estimate the true mean number of fans that show up for the women’s games.\nAfter collecting the raw data, Brennan presents a graph to Laura. The attendance appears normally distributed, its curve resembling that of the bell shape typical of a normal distribution, with a lot of games having values somewhere around 600. But there are also some with lower attendance, and some with attendance in the thousands! Laura thinks it would be great if every home game could get close to 1000. Looking at the graph, Laura tells Brennan that our true population mean is probably somewhere in the middle based on the graph. He tells Laura that it probably is, but in order to make sure we are going to use the maximum likelihood estimation technique to try and estimate this parameter of the normal distribution. “We’re going to try and find an equation that will be our estimator for mu and given some values this equation will maximize our likelihood function,” starts Brennan, “we also want to make sure that our estimator is not biased, meaning we aren’t going to be undershooting or overshooting our estimates. And we want to keep our estimates within a small range, hopefully close to our true mean, so ideally the variance is low too.” “Is there a way that we can measure that?” asks Laura. Brennan tells her that they can check if their estimator is consistent. “What does that even mean?” she asks. Brennan replies, “basically if our sample size was really huge, we want to know if the expected value of our estimator approaches the parameter and if our variance approaches zero.” Laura nods in understanding and they get to work on the math.\nTogether they find an estimator for the mean, input their values to get the estimate, and the pub cookies are requested. After the first game with cookies, word quickly spreads across campus about the delicious incentive. The promotion occurs occasionally over the next few seasons, and attendance only continues to rise. The year is 2030, and Brennan, now the head athletic director, decides to look at the attendance numbers for women’s hockey for every game from back in the 2025 season all the way up until now. He pulls up the data, the crooked smile on his face matching the left skewed distribution that appears on his screen."
  },
  {
    "objectID": "posts/reflection/index.html",
    "href": "posts/reflection/index.html",
    "title": "Reflecting on the Mini Projects",
    "section": "",
    "text": "A recap of all mini projects from the semester.\nIn the first mini project, we simulated the sampling distributions of sample minimums and maximums taken from different populations. This unit was essentially our introduction to test statistics in general, which became an important concept as the semester progressed. We used them in confidence intervals and we derived estimators for these statistics for distributions (ex. MLE for a mean). A lot of the semester involved different methods of trying to come up with the true parameters for distributions, which often used methods involving simulating samples. In mini project 3, we simulated samples from a population in order to find confidence intervals for p, which was essentially the mean, a test statistic, of a binomial sample. Overall, I feel like this project helped prepare me for the types of sampling simulations that we went on to do over the rest of the semester and highlighted the importance and usefulness of simulation in statistics. Also, based on the conclusions of this project relating to symmetry and SE, it showed how there are often patterns in statistics.\nIn the second mini project, we wrote a “meaningful story” using vocabulary from our unit on estimation. During this unit we worked on different methods of estimating parameters, and what may make certain estimators “better” than others. In this project we had a set of vocabulary that we had to use meaningfully in context, and some of those words made appearances in other course content. One of those words was “likelihood,” which we saw later in our unit on hypothesis testing using likelihood ratios. Having to use these words in a context that made sense helped me better understand the concepts themselves, as I really had to think about what the words meant so I could use them properly, or “meaningfully.” It also showed how many concepts we learn about are related. Given that a lot of these words popped back up in other units, this mini project definitely prepared me for that, ensuring that I understood at least the rudimentary idea behind them before going on to apply them in more complex ways. This connects with mini project 5, where we read an editorial about how we should and shouldn’t be using p-values. The editorial emphasized the idea of being thoughtful and intentional with our analyses and when drawing conclusions, a theme that resonated in the “meaningful story.”\nIn the third mini project, we simulated confidence intervals with different sample sizes to investigate what happens when assumptions are violated. The main technical aspect of this project, the simulation, ties back into that general theme of the usefulness and importance of simulation in statistics. In this project we were investigating more the properties of confidence intervals, and this relates to the section we covered later on credible intervals for parameters we were looking at using Bayesian statistics. In saying that, this project related to mini project 4, where we came up with credible intervals for a proportion. Having this background information on how confidence intervals actually work (and what we’re actually “confident” in) definitely helped prepare me for interpreting the Bayesian equivalent of the confidence interval, the credible interval. This was also like in mini project 1 where we were examining properties (SE) for different distributions, except in this case it was examining properties for different n/sample size of the same distribution. In general, after doing this project I definitely had a better understanding of why we make certain assumptions, what happens if we don’t fulfill them, and the real meaning when we interpret confidence intervals.\nIn the fourth mini project, we used Bayesian analysis to try and come up with a distribution to model the probability of winning points in tennis. A large part of this whole unit was working through the transition from the frequentist to Bayesian framework. Like in the rest of our units prior to this one, this was us trying to model a distribution, just in a different way. Whether it was estimation, sampling distributions, or confidence intervals, we had essentially been trying to model true population values of some sort, and this mini project was exactly that too. The “real-world” context of this project also reminded me of our meaningful story from project 2, in that we were using all these words we’ve learned, but now in the context of solving a specific (tennis) problem. Something that stood out to me when learning about the Bayesian framework in general is the idea of informative priors and how they can be subjective. Statistics and math are subjects that I often think of as fairly objective/cut and dry, the tried and true methods always working. But in this case, our end result very much depends on what we choose as a prior, and that’s something different than what I’ve experienced before. It is also interesting as it could introduce personal bias to models, as people may have different perceptions.\nIn the fifth and final mini project, we read the editorial “Moving to a World Beyond p&lt;0.05.” The main idea was to get readers to re-think how we use p-values. In every statistics course I’ve taken, when testing something for “significance,” we used a p-value and a threshold of 0.05 (typically) to determine this. Tying back to the theme of being meaningful and thoughtful, the editorial also mentions rethinking how confidence might not be the best word for what we refer to as confidence intervals, instead suggesting the world compatibility. Tying this back into our coursework and mini project surrounding confidence intervals, this actually makes sense. We generate these intervals by simulating a specific set of data, so encouraging statistical thinking and thoughtfulness by taking things like data context into account is important. When using confidence intervals for things like difference of means, answering questions like “is there a difference”, we often check if the interval contains 0, and base our conclusions off that, which is another area we could be “thoughtful” in. Overall, this project really instilled in me the importance of being intentional in analysis and understanding what I’m actually doing, which has been a common theme throughout this course.\nOver the course of the semester I have definitely learned a lot, both content wise and about what it means to be a good statistician (or anyone who works with data). These mini projects allowed me to take what we learned in class and apply it in a way that both bettered my understanding of these topics, and showed me how they may be used in practice. There were definitely some overarching themes, both technically and otherwise. In general, it showed the power of simulation and how it is behind a lot of statistical tests we use. While what we learn in class often seems very theoretical and derivation heavy, these projects show that there are practical applications (ex. Bayesian). All of the projects and concepts we covered also build off of each other in one way or another, some in more subtle ways, and that’s something that will stick with me as I continue in my academic and professional career. If I don’t know something, I can probably build off of something I do understand. I will also make sure to be be intentional and thoughtful with my work, whether it is related to statistics/data science or not."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math Stat Blog",
    "section": "",
    "text": "Reflecting on the Mini Projects\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nTaylor Lum\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 5: Advantages and Drawbacks of Using p-values\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nTaylor Lum\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 4: Bayesian Analysis\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nTaylor Lum\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 3: Simulation to Investigate Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nTaylor Lum\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 2: A Meaningful Story\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nTaylor Lum\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 1: Sampling Distribution of the Sample Minimum and Maximum\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nTaylor Lum\n\n\n\n\n\n\nNo matching items"
  }
]